{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Binance financial data Cardano\n",
    "In this notebook we will use the 'python-binance' package to extract financial data from Binance' API. For this to work one should have their own Binance account and request an API key and secret password. We are looking to extract financial data from Cardano with different time frames (1m, 5m, 1h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-binance\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import os.path\n",
    "import time\n",
    "from binance.client import Client\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "### API\n",
    "\n",
    "binance_api_key = '[YOUR API KEY HERE]'\n",
    "binance_api_secret = '[YOUR SECRET API PASSWORD HERE]'\n",
    "\n",
    "### CONSTANTS\n",
    "binsizes = {\"1m\": 1, \"5m\": 5, \"1h\": 60}\n",
    "batch_size = 750\n",
    "binance_client = Client(api_key=binance_api_key, api_secret=binance_api_secret)\n",
    "\n",
    "\n",
    "### FUNCTIONS\n",
    "def minutes_of_new_data(symbol, kline_size, data, source):\n",
    "    if len(data) > 0:  old = parser.parse(data[\"timestamp\"].iloc[-1])\n",
    "    elif source == \"binance\": old = datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
    "    if source == \"binance\": new = pd.to_datetime(binance_client.get_klines(symbol=symbol, interval=kline_size)[-1][0], unit='ms')\n",
    "    return old, new\n",
    "\n",
    "def get_all_binance(symbol, kline_size, save = False):\n",
    "    filename = '%s-%s-data.csv' % (symbol, kline_size)\n",
    "    if os.path.isfile(filename): data_df = pd.read_csv(filename)\n",
    "    else: data_df = pd.DataFrame()\n",
    "    oldest_point, newest_point = minutes_of_new_data(symbol, kline_size, data_df, source = \"binance\")\n",
    "    delta_min = (newest_point - oldest_point).total_seconds()/60\n",
    "    available_data = math.ceil(delta_min/binsizes[kline_size])\n",
    "    if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'): print('Downloading all available %s data for %s. Be patient..!' % (kline_size, symbol))\n",
    "    else: print('Downloading %d minutes of new data available for %s, i.e. %d instances of %s data.' % (delta_min, symbol, available_data, kline_size))\n",
    "    klines = binance_client.get_historical_klines(symbol, kline_size, oldest_point.strftime(\"%d %b %Y %H:%M:%S\"), newest_point.strftime(\"%d %b %Y %H:%M:%S\"))\n",
    "    data = pd.DataFrame(klines, columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore' ])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
    "    if len(data_df) > 0:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        data_df = data_df.append(temp_df)\n",
    "    else: data_df = data\n",
    "    data_df.set_index('timestamp', inplace=True)\n",
    "    if save: data_df.to_csv(filename)\n",
    "    print('All caught up..!')\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading all available 1m data for ADAUSDT. Be patient..!\n",
      "All caught up..!\n",
      "Downloading all available 5m data for ADAUSDT. Be patient..!\n",
      "All caught up..!\n",
      "Downloading all available 1h data for ADAUSDT. Be patient..!\n",
      "All caught up..!\n"
     ]
    }
   ],
   "source": [
    "DOTUSDT_1m = get_all_binance('ADAUSDT', \"1m\", save = True )\n",
    "DOTUSDT_5m = get_all_binance('ADAUSDT', \"5m\", save = True )\n",
    "DOTUSDT_1h = get_all_binance('ADAUSDT', \"1h\", save = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually inspecting the datasets it became clear that there is data missing in the 1m and 1h dataset on 30/11/2020, 21/12/2020 and 11/02/2021. The missing data corresponds with announced maintenance. Therefore all missing timeframes are manually inserted with 0 trading volume etc. and a non-changing start, high, low, end prices. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
